# 无人机环境使用指南与强化学习训练

## 1. 环境概述

`gym-pybullet-drones`是一个基于PyBullet的无人机强化学习环境，提供了OpenAI Gym接口，支持单智能体和多智能体强化学习训练。该环境具有以下特点：

- 基于PyBullet物理引擎，支持真实的无人机动力学模拟
- 支持多种无人机模型（CF2X、CF2P、HB）
- 提供多种环境类型（起飞、悬停、穿越门等）
- 支持多种动作空间和观察空间
- 支持多种强化学习算法（A2C、PPO、SAC、TD3、DDPG等）

## 2. 环境结构

### 2.1 核心组件

- **BaseAviary**：所有环境的基类，处理物理模拟和渲染
- **BaseSingleAgentAviary**：单智能体环境的基类
- **TakeoffAviary**：起飞环境
- **HoverAviary**：悬停环境
- **FlyThruGateAviary**：穿越门环境
- **TuneAviary**：PID参数调优环境

### 2.2 动作空间（ActionType）

| 类型 | 描述 | 动作维度 |
|------|------|---------|
| RPM | 直接控制四个电机的转速 | 4 |
| DYN | 控制期望的推力和力矩 | 4 |
| PID | 使用PID控制器控制位置 | 3 |
| VEL | 控制速度（使用PID控制器） | 4 |
| TUN | 调整PID控制器参数 | 6 |
| ONE_D_RPM | 简化的一维动作（所有电机相同） | 1 |
| ONE_D_DYN | 简化的一维推力控制 | 1 |
| ONE_D_PID | 简化的一维位置控制 | 1 |

### 2.3 观察空间（ObservationType）

| 类型 | 描述 | 观察维度 |
|------|------|---------|
| KIN | 运动学信息 | 12（位置、姿态、速度、角速度） |
| RGB | RGB相机图像 | (48, 64, 4) |

## 3. 环境使用方法

### 3.1 基本使用

```python
import gymnasium as gym
import gym_pybullet_drones

# 创建环境
env = gym.make("takeoff-aviary-v0")

# 重置环境
obs = env.reset()

# 交互循环
for _ in range(1000):
    # 随机选择动作
    action = env.action_space.sample()
    # 执行动作
    obs, reward, done, info = env.step(action)
    # 渲染（可选）
    env.render()
    # 检查是否结束
    if done:
        obs = env.reset()

# 关闭环境
env.close()
```

### 3.2 自定义环境参数

```python
from gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary import TakeoffAviary
from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import ActionType, ObservationType

# 创建自定义参数的环境
env = TakeoffAviary(
    drone_model="cf2x",
    initial_xyzs=[[0, 0, 0.1]],  # 初始位置
    initial_rpys=[[0, 0, 0]],    # 初始姿态
    physics="pyb",              # 物理引擎
    freq=240,                   # 模拟频率
    aggregate_phy_steps=1,      # 每次step的物理更新次数
    gui=True,                   # 是否显示GUI
    record=False,               # 是否录制视频
    obs=ObservationType.KIN,    # 观察类型
    act=ActionType.RPM          # 动作类型
)
```

## 4. 强化学习训练

### 4.1 使用stable-baselines3

```python
import gymnasium as gym
import gym_pybullet_drones
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env

# 创建环境
env = gym.make("hover-aviary-v0")

# 检查环境
check_env(env, warn=True, skip_render_check=True)

# 定义模型
model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    tensorboard_log="./logs/"
)

# 训练模型
model.learn(total_timesteps=100000)

# 保存模型
model.save("./models/ppo_hover")

# 加载模型
model = PPO.load("./models/ppo_hover")

# 测试模型
env = gym.make("hover-aviary-v0", gui=True)
obs = env.reset()
for _ in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    if done:
        obs = env.reset()

env.close()
```

### 4.2 使用ray[rllib]

```python
import ray
from ray import tune
from ray.rllib.agents import ppo
from gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary import TakeoffAviary

# 初始化Ray
ray.shutdown()
ray.init(ignore_reinit_error=True)

# 注册环境
tune.register_env("takeoff-aviary-v0", lambda _: TakeoffAviary())

# 配置PPO算法
config = ppo.DEFAULT_CONFIG.copy()
config["num_workers"] = 2
config["framework"] = "torch"
config["env"] = "takeoff-aviary-v0"

# 创建训练器
agent = ppo.PPOTrainer(config)

# 训练模型
for i in range(10):
    results = agent.train()
    print(f"[{i}]: episode_reward_mean={results['episode_reward_mean']}")

# 保存模型
agent.save("./models/rllib_ppo_takeoff")

# 关闭Ray
ray.shutdown()
```

### 4.3 使用官方训练脚本

```bash
# 单智能体训练
cd gym-pybullet-drones/experiments/learning/
python3 singleagent.py --env hover --algo ppo --obs kin --act rpm --cpu 4

# 多智能体训练
python3 multiagent.py --num_drones 2 --env takeoff --obs kin --act rpm --algo ppo --num_workers 4
```

## 5. 环境扩展

### 5.1 创建自定义环境

```python
from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import BaseSingleAgentAviary

class CustomAviary(BaseSingleAgentAviary):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # 自定义初始化

    def _computeReward(self):
        # 自定义奖励函数
        state = self._getDroneStateVector(0)
        return -np.linalg.norm(state[0:3] - np.array([1, 1, 1]))  # 目标位置[1,1,1]

    def _computeDone(self):
        # 自定义终止条件
        state = self._getDroneStateVector(0)
        if np.linalg.norm(state[0:3] - np.array([1, 1, 1])) < 0.1:
            return True
        elif self.step_counter/self.SIM_FREQ > self.EPISODE_LEN_SEC:
            return True
        else:
            return False

# 使用自定义环境
env = CustomAviary(gui=True, obs=ObservationType.KIN, act=ActionType.RPM)
```

### 5.2 自定义动作空间

```python
from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import BaseSingleAgentAviary, ActionType

class CustomActionAviary(BaseSingleAgentAviary):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.ACT_TYPE = ActionType("custom")  # 自定义动作类型

    def _actionSpace(self):
        # 自定义动作空间
        return spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float32)

    def _preprocessAction(self, action):
        # 自定义动作预处理
        # 将自定义动作转换为电机转速
        thrust = self.GRAVITY * (1 + 0.5 * action[0])
        yaw_rate = 0.5 * action[1]
        # 使用nnlsRPM将推力和力矩转换为转速
        return nnlsRPM(thrust=thrust, x_torque=0, y_torque=0, z_torque=yaw_rate, ...)
```

## 6. 注意事项

1. **性能优化**：
   - 使用`gui=False`和`aggregate_phy_steps>1`可以提高性能
   - 对于大规模训练，建议使用多个环境并行训练

2. **调试技巧**：
   - 使用`gui=True`和`user_debug_gui=True`可以可视化环境
   - 使用`ONE_D_*`动作空间可以简化调试

3. **参数调整**：
   - 调整`EPISODE_LEN_SEC`可以改变每轮的时间长度
   - 调整`freq`和`aggregate_phy_steps`可以平衡模拟精度和性能

4. **可视化结果**：
   - 使用Tensorboard可以可视化训练过程：`tensorboard --logdir ./logs/`
   - 使用`record=True`可以录制模拟视频

## 7. 常见问题

### 7.1 环境安装问题

```bash
# 确保已安装所有依赖
pip install --upgrade numpy Pillow matplotlib cycler gym pybullet stable_baselines3 'ray[rllib]'

# 安装环境
pip install -e .
```

### 7.2 Gym版本问题

```bash
# 如果遇到Gym版本问题，可以尝试安装特定版本
pip install gymnasium==0.26.2
```

### 7.3 渲染问题

```bash
# 确保已安装正确的图形驱动
# 对于Linux，确保已安装OpenGL
```

## 8. 示例项目

### 8.1 起飞训练

```bash
cd gym-pybullet-drones/examples/
python3 learn.py
```

### 8.2 悬停训练

```bash
cd gym-pybullet-drones/experiments/learning/
python3 singleagent.py --env hover --algo ppo --obs kin --act rpm --cpu 4
```

### 8.3 穿越门训练

```bash
python3 singleagent.py --env flythrugate --algo ppo --obs rgb --act pid --cpu 1
```

## 9. 总结

`gym-pybullet-drones`是一个功能强大的无人机强化学习环境，提供了丰富的环境类型、动作空间和观察空间，支持多种强化学习算法。通过本文的指南，您可以快速上手使用该环境进行无人机强化学习训练，并根据需要进行自定义扩展。

祝您训练愉快！